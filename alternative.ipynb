{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a5a5719-262c-4231-975b-7fde511b5c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 18:57:14.287667: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-15 18:57:20.594852: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfs/space/software/cluster_software/spack/linux-centos7-x86_64/gcc-9.2.0/cudnn-8.2.0.53-11.3-3qb34ykh5rffuzu6j3rl4tm6yarrjg7w/lib64:/gpfs/space/software/cluster_software/spack/linux-centos7-x86_64/gcc-9.2.0/cuda-11.3.1-oqzddj7nezymwww6ennwec7qb6kktktw/lib64::/usr/local/cuda-12.6/lib64\n",
      "2024-12-15 18:57:20.595033: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfs/space/software/cluster_software/spack/linux-centos7-x86_64/gcc-9.2.0/cudnn-8.2.0.53-11.3-3qb34ykh5rffuzu6j3rl4tm6yarrjg7w/lib64:/gpfs/space/software/cluster_software/spack/linux-centos7-x86_64/gcc-9.2.0/cuda-11.3.1-oqzddj7nezymwww6ennwec7qb6kktktw/lib64::/usr/local/cuda-12.6/lib64\n",
      "2024-12-15 18:57:20.595042: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras_core as keras\n",
    "import keras_nlp\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44ea0e5a-62da-48a6-afda-235df01e4665",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "#NUM_TRAINING_EXAMPLES = df_train.shape[0]\n",
    "TRAIN_SPLIT = 0.8\n",
    "VAL_SPLIT = 0.2\n",
    "#STEPS_PER_EPOCH = int(NUM_TRAINING_EXAMPLES)*TRAIN_SPLIT // BATCH_SIZE\n",
    "EPOCHS = 2\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "959dbee4-d262-4987-8d5e-bc8921ab43d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train = \"train.csv\", test = \"test.csv\"):\n",
    "    df_train = pd.read_csv(train)\n",
    "    df_test = pd.read_csv(test)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(df_train[\"text\"], df_train[\"target\"], test_size = 0.2, random_state=42)\n",
    "    X_test = df_test[\"text\"]\n",
    "    test_ids = df_test[\"id\"]\n",
    "\n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, test_ids)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ea81e08-c55b-40ba-904e-be8c50c6b6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preprocessor(model_name: str, sequence_length: int = 160):\n",
    "    if 'distil' in model_name.lower():\n",
    "        return keras_nlp.models.DistilBertPreprocessor.from_preset(\n",
    "            model_name,\n",
    "            sequence_length=sequence_length\n",
    "        )\n",
    "    elif 'roberta' in model_name.lower():\n",
    "        return keras_nlp.models.RobertaPreprocessor.from_preset(\n",
    "            model_name,\n",
    "            sequence_length=sequence_length\n",
    "        )\n",
    "    elif 'bert' in model_name.lower():\n",
    "        return keras_nlp.models.BertPreprocessor.from_preset(\n",
    "            model_name,\n",
    "            sequence_length=sequence_length\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc1543a7-00e5-4ebb-bd3b-c453693061d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_backbone(model_name: str):\n",
    "    if 'distil' in model_name.lower():\n",
    "        return keras_nlp.models.DistilBertBackbone.from_preset(model_name)\n",
    "    elif 'roberta' in model_name.lower():\n",
    "        return keras_nlp.models.RobertaBackbone.from_preset(model_name)\n",
    "    elif 'bert' in model_name.lower():\n",
    "        return keras_nlp.models.BertBackbone.from_preset(model_name)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ad394ee-b21a-4ae6-b464-ac425b544ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_classifier_model(backbone, preprocessor, hidden_dims=64):\n",
    "    \"\"\"Create a custom classification model.\"\"\"\n",
    "    # Create input layers\n",
    "    input_text = keras.layers.Input(shape=(), dtype=tf.string, name=\"text\")\n",
    "    \n",
    "    # Preprocess the text\n",
    "    preprocessed = preprocessor(input_text)\n",
    "    \n",
    "    # Get backbone features\n",
    "    sequence_output = backbone(preprocessed)\n",
    "    \n",
    "    # Pool the sequence output\n",
    "    pooled = keras.layers.GlobalAveragePooling1D()(sequence_output)\n",
    "    \n",
    "    # Add classification layers\n",
    "    hidden = keras.layers.Dense(hidden_dims, activation=\"relu\")(pooled)\n",
    "    hidden = keras.layers.Dropout(0.1)(hidden)\n",
    "    outputs = keras.layers.Dense(2)(hidden)\n",
    "    \n",
    "    # Create model\n",
    "    model = keras.Model(inputs=input_text, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b54091a1-a974-4290-b571-b707f7f31841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_default_classifier(preprocessor):\n",
    "    preset = \"distil_bert_base_en_uncased\"\n",
    "    classifier = keras_nlp.models.DistilBertClassifier.from_preset(preset,\n",
    "                                                               preprocessor = preprocessor, \n",
    "                                                               num_classes=2)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c686493-c039-4463-b71f-982d9d566e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_val, y_val, epochs=2, batch_size=32):\n",
    "    f1 = keras.metrics.F1Score(\n",
    "        threshold=0.5,\n",
    "        average=\"macro\",\n",
    "        name=\"f1\")\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=keras.optimizers.Adam(1e-5),\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    # Train - note that the model will handle preprocessing internally\n",
    "    # since we included the preprocessor in the model definition\n",
    "    history = model.fit(\n",
    "        x=X_train,\n",
    "        y=y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_val, y_val)\n",
    "    )\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b4fe046-8fd0-4f37-9293-50833e305c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(model, X_test, test_ids, output_path=\"submission.csv\"):\n",
    "   # Convert to numpy array if needed\n",
    "        \n",
    "    predictions = model.predict(X_test)\n",
    "    binary_predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_ids,\n",
    "        'target': binary_predictions\n",
    "    })\n",
    "    \n",
    "    submission.to_csv(output_path, index=False)\n",
    "    print(f\"Submission saved to {output_path}\")\n",
    "    \n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "676bfaa0-53d7-4245-94d5-a7818ed801a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "AVAILABLE_MODELS = {\n",
    "    'distilbert': 'distil_bert_base_en_uncased',\n",
    "    'bert': 'bert_base_en_uncased',\n",
    "    'roberta': 'roberta_base_en',\n",
    "}\n",
    "\n",
    "def run_experiment(model_name: str):\n",
    "    \"\"\"Run a complete experiment with specified model.\"\"\"\n",
    "    print(f\"\\nRunning experiment with {model_name}\")\n",
    "    preset = AVAILABLE_MODELS[model_name]\n",
    "    \n",
    "    # 1. Load data\n",
    "    (X_train, y_train), (X_val, y_val), (X_test, test_ids) = load_data(\"train.csv\", \"test.csv\")\n",
    "    print(\"Data loaded\")\n",
    "    \n",
    "    # 2. Create components\n",
    "    preprocessor = create_preprocessor(preset)\n",
    "    #backbone = create_backbone(preset)\n",
    "    model = create_default_classifier(preprocessor)\n",
    "    print(\"Model components created\")\n",
    "    \n",
    "    \n",
    "    # 3. Train\n",
    "    history = train_model(model, X_train, y_train, X_val, y_val)\n",
    "    print(\"Training completed\")\n",
    "    \n",
    "    # 4. Create submission\n",
    "    submission = create_submission(\n",
    "        model, \n",
    "        X_test, \n",
    "        test_ids, \n",
    "        f\"submission_{model_name}.csv\"\n",
    "    )\n",
    "    \n",
    "    return model, history, submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6e6ca8a-9617-4ddb-b557-a715929239db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running experiment with distilbert\n",
      "Data loaded\n",
      "Model components created\n",
      "WARNING:tensorflow:From /gpfs/helios/home/fox4609/mlcourse/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "Epoch 1/2\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 312ms/step - accuracy: 0.7184 - loss: 0.5646 - val_accuracy: 0.8431 - val_loss: 0.3927\n",
      "Epoch 2/2\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 154ms/step - accuracy: 0.8499 - loss: 0.3763 - val_accuracy: 0.8418 - val_loss: 0.3822\n",
      "Training completed\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 78ms/step\n",
      "Submission saved to submission_distilbert.csv\n",
      "\n",
      "Final metrics:\n",
      "Val accuracy: 0.8418\n",
      "Val loss: 0.3822\n"
     ]
    }
   ],
   "source": [
    "model, history, submission = run_experiment('distilbert')\n",
    "print(\"\\nFinal metrics:\")\n",
    "print(f\"Val accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "print(f\"Val loss: {history.history['val_loss'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d6ceec0-75b4-4fc1-8fb3-404f77161687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n",
      "\n",
      "Final shapes in dataset:\n",
      "Token ids shape: (16, 160)\n",
      "Keyword shape: (16, 1, 223)\n",
      "Location shape: (16, 1, 3343)\n",
      "Epoch 1/2\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 179ms/step - accuracy: 0.6751 - loss: 12.6997 - val_accuracy: 0.8435 - val_loss: 11.2568\n",
      "Epoch 2/2\n",
      "\u001b[1m  2/238\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 75ms/step - accuracy: 0.9375 - loss: 11.1494"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/space/software/jupyterhub/python3.9-rhel9/lib/python3.9/contextlib.py:137: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 166ms/step - accuracy: 0.8451 - loss: 10.9000 - val_accuracy: 0.8542 - val_loss: 9.8368\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# First load full dataset\n",
    "train_full = tf.data.experimental.make_csv_dataset(\n",
    "    \"train.csv\",\n",
    "    batch_size=16,\n",
    "    label_name='target',\n",
    "    select_columns=['text', 'keyword', 'location', 'target'], \n",
    "    num_epochs=10\n",
    ")\n",
    "print(\"Started\")\n",
    "# Create preprocessors\n",
    "preprocessor = keras_nlp.models.DistilBertPreprocessor.from_preset(\n",
    "    \"distil_bert_base_en_uncased\",\n",
    "    sequence_length=160\n",
    ")\n",
    "\n",
    "keyword_lookup = keras.layers.StringLookup(output_mode=\"multi_hot\")\n",
    "location_lookup = keras.layers.StringLookup(output_mode=\"multi_hot\")\n",
    "\n",
    "# Adapt lookups\n",
    "keyword_data = train_full.map(lambda x, y: x['keyword'])\n",
    "location_data = train_full.map(lambda x, y: x['location'])\n",
    "keyword_lookup.adapt(keyword_data)\n",
    "location_lookup.adapt(location_data)\n",
    "\n",
    "keyword_size = len(keyword_lookup.get_vocabulary())\n",
    "location_size = len(location_lookup.get_vocabulary())\n",
    "\n",
    "# Split data\n",
    "train_data = train_full.shuffle(buffer_size=1000)\n",
    "val_size = len(list(train_full)) // 5\n",
    "val_ds = train_data.take(val_size)\n",
    "train_ds = train_data.skip(val_size)\n",
    "train_size = len(list(train_full)) - val_size\n",
    "batch_size = 16\n",
    "steps_per_epoch = train_size // batch_size\n",
    "\n",
    "def prepare_data(features, label):\n",
    "    text_preprocessed = preprocessor(features['text'])\n",
    "    \n",
    "    # Use map_fn to apply lookup to each element in the batch\n",
    "    keyword_encoded = tf.map_fn(\n",
    "        keyword_lookup, \n",
    "        features['keyword'],\n",
    "        fn_output_signature=tf.float32\n",
    "    )\n",
    "    location_encoded = tf.map_fn(\n",
    "        location_lookup, \n",
    "        features['location'],\n",
    "        fn_output_signature=tf.float32\n",
    "    )\n",
    "    \n",
    "    # Add middle dimension\n",
    "    keyword_encoded = tf.expand_dims(keyword_encoded, axis=1)\n",
    "    location_encoded = tf.expand_dims(location_encoded, axis=1)\n",
    "    \n",
    "    features_dict = {\n",
    "        'token_ids': text_preprocessed['token_ids'],\n",
    "        'padding_mask': text_preprocessed['padding_mask'],\n",
    "        'keyword_encoding': keyword_encoded,\n",
    "        'location_encoding': location_encoded\n",
    "    }\n",
    "    return features_dict, label\n",
    "\n",
    "# Apply the data preparation\n",
    "train_ds = train_ds.map(prepare_data)\n",
    "val_ds = val_ds.map(prepare_data)\n",
    "\n",
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(tf.data.AUTOTUNE)\n",
    "train_size = len(list(train_full)) - val_size\n",
    "batch_size = 16\n",
    "steps_per_epoch = train_size // batch_size\n",
    "# Verify shapes\n",
    "for features, label in train_ds.take(1):\n",
    "    print(\"\\nFinal shapes in dataset:\")\n",
    "    print(\"Token ids shape:\", features['token_ids'].shape)\n",
    "    print(\"Keyword shape:\", features['keyword_encoding'].shape)\n",
    "    print(\"Location shape:\", features['location_encoding'].shape)\n",
    "# Create model\n",
    "backbone = keras_nlp.models.DistilBertBackbone.from_preset(\n",
    "    \"distil_bert_base_en_uncased\"\n",
    ")\n",
    "\n",
    "backbone_inputs = backbone.input\n",
    "keyword_input = keras.layers.Input(shape=(1, keyword_size), dtype=tf.float32, name=\"keyword_encoding\")\n",
    "location_input = keras.layers.Input(shape=(1, location_size), dtype=tf.float32, name=\"location_encoding\")\n",
    "\n",
    "x = backbone(backbone_inputs)\n",
    "x = keras.layers.GlobalAveragePooling1D()(x)\n",
    "x = keras.layers.Reshape((1, 768))(x)\n",
    "\n",
    "combined = keras.layers.Concatenate(axis=-1)([x, keyword_input, location_input])\n",
    "combined = keras.layers.Flatten()(combined)\n",
    "\n",
    "x = keras.layers.Dense(512, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.01))(combined)\n",
    "x = keras.layers.Dropout(0.3)(x)\n",
    "x = keras.layers.Dense(256, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.01))(x)\n",
    "x = keras.layers.Dropout(0.3)(x)\n",
    "outputs = keras.layers.Dense(2)(x)\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs={**backbone_inputs, \n",
    "            'keyword_encoding': keyword_input,\n",
    "            'location_encoding': location_input},\n",
    "    outputs=outputs\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.Adam(1e-5),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10,  # added epochs\n",
    "    steps_per_epoch = steps_per_epoch\n",
    ")\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ef84c0e-086f-47ed-bb4d-8ab81ae7910a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load test data as a pandas DataFrame first\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m test_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Convert data types and handle missing values\u001b[39;00m\n\u001b[1;32m      5\u001b[0m test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Load test data as a pandas DataFrame first\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Convert data types and handle missing values\n",
    "test_df['text'] = test_df['text'].fillna('')\n",
    "test_df['keyword'] = test_df['keyword'].fillna('')\n",
    "test_df['location'] = test_df['location'].fillna('')\n",
    "\n",
    "# Create dataset using make_csv_dataset but with fixed batch size and no shuffling\n",
    "test_ds = tf.data.experimental.make_csv_dataset(\n",
    "    \"test.csv\",\n",
    "    batch_size=16,\n",
    "    label_name=None,\n",
    "    select_columns=['text', 'keyword', 'location'],\n",
    "    shuffle=False,\n",
    "    num_epochs=1\n",
    ")\n",
    "\n",
    "# Create ordered list of IDs\n",
    "test_ids = test_df['id'].tolist()\n",
    "\n",
    "# Apply preprocessing\n",
    "test_ds = test_ds.map(prepare_test_data)\n",
    "\n",
    "# Get predictions\n",
    "all_predictions = []\n",
    "for batch in test_ds:\n",
    "    batch_preds = model.predict_on_batch(batch)\n",
    "    all_predictions.extend(tf.argmax(batch_preds, axis=1).numpy())\n",
    "\n",
    "# Create submission DataFrame ensuring length matches\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_ids[:len(all_predictions)],\n",
    "    'target': all_predictions\n",
    "})\n",
    "\n",
    "# Verify the submission\n",
    "print(\"Test file shape:\", test_df.shape)\n",
    "print(\"Submission shape:\", submission.shape)\n",
    "print(\"Predictions distribution:\")\n",
    "print(pd.Series(all_predictions).value_counts())\n",
    "\n",
    "# Save submission\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe736fca-7dd7-4f72-85ab-ac0ae6e4162a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlcourse",
   "language": "python",
   "name": "mlcourse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
