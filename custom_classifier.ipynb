{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fa1b49c-94b5-4f6c-af15-6aba2bc0aeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-16 11:57:39.307676: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-16 11:57:40.186306: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfs/space/software/cluster_software/spack/linux-centos7-x86_64/gcc-9.2.0/cudnn-8.2.0.53-11.3-3qb34ykh5rffuzu6j3rl4tm6yarrjg7w/lib64:/gpfs/space/software/cluster_software/spack/linux-centos7-x86_64/gcc-9.2.0/cuda-11.3.1-oqzddj7nezymwww6ennwec7qb6kktktw/lib64::/usr/local/cuda-12.6/lib64\n",
      "2024-12-16 11:57:40.186474: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfs/space/software/cluster_software/spack/linux-centos7-x86_64/gcc-9.2.0/cudnn-8.2.0.53-11.3-3qb34ykh5rffuzu6j3rl4tm6yarrjg7w/lib64:/gpfs/space/software/cluster_software/spack/linux-centos7-x86_64/gcc-9.2.0/cuda-11.3.1-oqzddj7nezymwww6ennwec7qb6kktktw/lib64::/usr/local/cuda-12.6/lib64\n",
      "2024-12-16 11:57:40.186483: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras_core as keras\n",
    "import keras_nlp\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d118753-5411-48e5-8c7e-6ff7217d5380",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wordnet\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f52952ca-2316-4f77-a431-d8b4a8367994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'augment_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m val_ds \u001b[38;5;241m=\u001b[39m train_data\u001b[38;5;241m.\u001b[39mtake(val_size)\n\u001b[1;32m     44\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m train_data\u001b[38;5;241m.\u001b[39mskip(val_size)\n\u001b[0;32m---> 46\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m train_ds\u001b[38;5;241m.\u001b[39mmap(\u001b[43maugment_batch\u001b[49m)\n\u001b[1;32m     48\u001b[0m train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(train_full)) \u001b[38;5;241m-\u001b[39m val_size\n\u001b[1;32m     49\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'augment_batch' is not defined"
     ]
    }
   ],
   "source": [
    "# First load full dataset\n",
    "train_full = tf.data.experimental.make_csv_dataset(\n",
    "    \"train.csv\",\n",
    "    batch_size=16,\n",
    "    label_name='target',\n",
    "    select_columns=['text', 'keyword', 'location', 'target'], \n",
    "    num_epochs=10\n",
    ")\n",
    "print(\"Started\")\n",
    "# Create preprocessors\n",
    "preprocessor = keras_nlp.models.DistilBertPreprocessor.from_preset(\n",
    "    \"distil_bert_base_en_uncased\",\n",
    "    sequence_length=160\n",
    ")\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=2,\n",
    "    min_delta=0.001,   \n",
    "    mode='min',     \n",
    "    restore_best_weights=True \n",
    ")\n",
    "keyword_lookup = keras.layers.StringLookup(output_mode=\"multi_hot\")\n",
    "location_lookup = keras.layers.StringLookup(output_mode=\"multi_hot\")\n",
    "\n",
    "# Adapt lookups\n",
    "keyword_data = train_full.map(lambda x, y: x['keyword'])\n",
    "location_data = train_full.map(lambda x, y: x['location'])\n",
    "keyword_lookup.adapt(keyword_data)\n",
    "location_lookup.adapt(location_data)\n",
    "\n",
    "keyword_size = len(keyword_lookup.get_vocabulary())\n",
    "location_size = len(location_lookup.get_vocabulary())\n",
    "\n",
    "# Split data\n",
    "train_data = train_full.shuffle(buffer_size=1000)\n",
    "val_size = len(list(train_full)) // 5\n",
    "val_ds = train_data.take(val_size)\n",
    "train_ds = train_data.skip(val_size)\n",
    "\n",
    "train_size = len(list(train_full)) - val_size\n",
    "batch_size = 16\n",
    "steps_per_epoch = train_size // batch_size\n",
    "\n",
    "def prepare_data(features, label):\n",
    "    text_preprocessed = preprocessor(features['text'])\n",
    "    \n",
    "    # Use map_fn to apply lookup to each element in the batch\n",
    "    keyword_encoded = tf.map_fn(\n",
    "        keyword_lookup, \n",
    "        features['keyword'],\n",
    "        fn_output_signature=tf.float32\n",
    "    )\n",
    "    location_encoded = tf.map_fn(\n",
    "        location_lookup, \n",
    "        features['location'],\n",
    "        fn_output_signature=tf.float32\n",
    "    )\n",
    "    \n",
    "    # Add middle dimension\n",
    "    keyword_encoded = tf.expand_dims(keyword_encoded, axis=1)\n",
    "    location_encoded = tf.expand_dims(location_encoded, axis=1)\n",
    "    \n",
    "    features_dict = {\n",
    "        'token_ids': text_preprocessed['token_ids'],\n",
    "        'padding_mask': text_preprocessed['padding_mask'],\n",
    "        'keyword_encoding': keyword_encoded,\n",
    "        'location_encoding': location_encoded\n",
    "    }\n",
    "    return features_dict, label\n",
    "\n",
    "# Apply the data preparation\n",
    "train_ds = train_ds.map(prepare_data)\n",
    "val_ds = val_ds.map(prepare_data)\n",
    "\n",
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(tf.data.AUTOTUNE)\n",
    "train_size = len(list(train_full)) - val_size\n",
    "batch_size = 16\n",
    "steps_per_epoch = train_size // batch_size\n",
    "\n",
    "# Create model\n",
    "backbone = keras_nlp.models.DistilBertBackbone.from_preset(\n",
    "    \"distil_bert_base_en_uncased\"\n",
    ")\n",
    "\n",
    "backbone_inputs = backbone.input\n",
    "keyword_input = keras.layers.Input(shape=(1, keyword_size), dtype=tf.float32, name=\"keyword_encoding\")\n",
    "location_input = keras.layers.Input(shape=(1, location_size), dtype=tf.float32, name=\"location_encoding\")\n",
    "\n",
    "x = backbone(backbone_inputs)\n",
    "x = keras.layers.GlobalAveragePooling1D()(x)\n",
    "x = keras.layers.Reshape((1, 768))(x)\n",
    "x = keras.layers.Dense(768, activation=\"gelu\")(x)\n",
    "x = keras.layers.Bidirectional(keras.layers.LSTM(384, return_sequences=True))(x)\n",
    "attention = keras.layers.MultiHeadAttention(\n",
    "    num_heads=8,\n",
    "    key_dim=48\n",
    ")(x, x)\n",
    "x = keras.layers.Add()([x, attention]) \n",
    "x = keras.layers.LayerNormalization()(x)\n",
    "\n",
    "combined = keras.layers.Concatenate(axis=-1)([x, keyword_input, location_input])\n",
    "combined = keras.layers.Flatten()(combined)\n",
    "\n",
    "x = keras.layers.Dense(512, activation=\"gelu\", kernel_regularizer=keras.regularizers.l2(0.01))(combined)\n",
    "x = keras.layers.Dropout(0.3)(x)\n",
    "x = keras.layers.Dense(256, activation=\"gelu\", kernel_regularizer=keras.regularizers.l2(0.01))(x)\n",
    "x = keras.layers.Dropout(0.3)(x)\n",
    "outputs = keras.layers.Dense(2)(x)\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs={**backbone_inputs, \n",
    "            'keyword_encoding': keyword_input,\n",
    "            'location_encoding': location_input},\n",
    "    outputs=outputs\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.Adam(1e-5),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10,\n",
    "    steps_per_epoch = steps_per_epoch,\n",
    "    callbacks = [early_stopping]\n",
    ")\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60c7300c-67f0-43ec-9727-569ad5319108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test file shape: (3263, 4)\n",
      "Submission shape: (3263, 2)\n",
      "Predictions distribution:\n",
      "0    1955\n",
      "1    1308\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "test_df['text'] = test_df['text'].fillna('')\n",
    "test_df['keyword'] = test_df['keyword'].fillna('')\n",
    "test_df['location'] = test_df['location'].fillna('')\n",
    "\n",
    "test_ds = tf.data.experimental.make_csv_dataset(\n",
    "    \"test.csv\",\n",
    "    batch_size=16,\n",
    "    label_name=None,\n",
    "    select_columns=['text', 'keyword', 'location'],\n",
    "    shuffle=False,\n",
    "    num_epochs=1\n",
    ")\n",
    "\n",
    "test_ids = test_df['id'].tolist()\n",
    "\n",
    "def prepare_test_data(features):\n",
    "    text_preprocessed = preprocessor(features['text'])\n",
    "\n",
    "    # Use map_fn to apply lookup to each element in the batch\n",
    "    keyword_encoded = tf.map_fn(\n",
    "        keyword_lookup, \n",
    "        features['keyword'],\n",
    "        fn_output_signature=tf.float32\n",
    "    )\n",
    "    location_encoded = tf.map_fn(\n",
    "        location_lookup, \n",
    "        features['location'],\n",
    "        fn_output_signature=tf.float32\n",
    "    )\n",
    "\n",
    "    # Add middle dimension\n",
    "    keyword_encoded = tf.expand_dims(keyword_encoded, axis=1)\n",
    "    location_encoded = tf.expand_dims(location_encoded, axis=1)\n",
    "\n",
    "    features_dict = {\n",
    "        'token_ids': text_preprocessed['token_ids'],\n",
    "        'padding_mask': text_preprocessed['padding_mask'],\n",
    "        'keyword_encoding': keyword_encoded,\n",
    "        'location_encoding': location_encoded\n",
    "    }\n",
    "    return features_dict\n",
    "    \n",
    "test_ds = test_ds.map(prepare_test_data)\n",
    "\n",
    "all_predictions = []\n",
    "for batch in test_ds:\n",
    "    batch_preds = model.predict_on_batch(batch)\n",
    "    all_predictions.extend(tf.argmax(batch_preds, axis=1).numpy())\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_ids[:len(all_predictions)],\n",
    "    'target': all_predictions\n",
    "})\n",
    "\n",
    "print(\"Test file shape:\", test_df.shape)\n",
    "print(\"Submission shape:\", submission.shape)\n",
    "print(\"Predictions distribution:\")\n",
    "print(pd.Series(all_predictions).value_counts())\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31954ac1-8513-4782-8585-d02af267938d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlcourse",
   "language": "python",
   "name": "mlcourse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
